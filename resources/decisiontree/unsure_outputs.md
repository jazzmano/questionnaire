If you are unsure whether your system is able to generate outputs such as predictions, content, recommendations, or decisions, you have two choices:

1. **Assume it *is* able to generate outputs (recommended).**

2. **Assume it *is not* able to generate outputs.**

We recommend you assume it *is* (option 1).

**Why?**  
*Why choose option 1?*
If you assume your system is able to generate outputs, it might be covered by the AI Act (if the other conditions for having an AI system are also fulfilled), but if it is not prohibited or high-risk, the rules you need to follow are usually simple. Therefore, this approach makes sure you are on the safe side. 

What really separates AI systems from regular IT systems, are AI systems ability to generate outputs. There are generally four broad categories of outputs: predictions, content, recommendations, and decisions.

    *Predictions* are estimates about unknown outcomes based on known inputs. For example, if you ask ChatGPT for investment advice on a particular stock, it can draw on its training data and possibly external sources (like the internet) to predict whether the stock may go up or down.

    *Content* refers to the generation of new material (generative AI). When you ask ChatGPT a question, it produces text, audio, images, or even video. All of these are considered AI-generated content.

    *Recommendations* are suggestions for specific actions, products, or services tailored to the specific user. For instance, if you use ChatGPT’s memory function, it can store your preferences. When you later seek advice, it can combine your chat history, stored preferences, and new inputs to provide you with a recommendation of what to do. 

    *Decisions* are choices made by an AI system that would traditionally require a human. For example, if an AI system in a bank evaluates loan applications and decides whether to grant credit to a customer, it is making a decision. 

It is important to know whether your system is capable of generating such outputs, as each type introduces its own risks. **Predicitions** can be wrong or biased, but users may rely on them as if they were facts. **Content** can be misleading and or infringe on intellectual property rights. **Recommendations** can change user behavior, potentially for the worse over time. **Decisions** can harm individuals if the system is flawed or biased.

For these reasons, it is critical for businesses to understand what types of outputs their AI systems are capable of generating. If you are unsure whether your system produces any of these outputs, or unsure which categories apply, the safest assumption is that your system may generate all types of outputs. You should then mitigate the potential risks associated with each accordingly.

By understanding and managing how your system generates outputs, you not only reduce legal risk, you also strengthen user trust and your organisation’s reputation in the market.

*Why choose option 2?*
You should choose Option 2 only if your system does not generate outputs. This applies, e.g., if your system only supports internal tasks or processes and does not produce outputs that are presented to users or used in a way that could influence their decisions, actions, or understanding.

Please note that many systems that appear purely internal or rule-based may still generate outputs that shape user behaviour or decisions indirectly. For example, a customer service platform may use AI to prioritise which support tickets agents should address first. While this output is not shown to customers directly, it affects which customers receive faster or slower service.

If you are unsure, it is safer to assume your system generates outputs and apply appropriate governance.

**When in doubt, assume your system can generate outputs.**  Better safe than sorry.
