If you are unsure whether your systemÂ infer, from the input it receives, how to generate outputs, you have two choices:

1. **Assume it *does* infer how to generate outputs (recommended).**

2. **Assume it *does not* infer how to generate outputs.**

We recommend you assume it *does* (option 1).

**Why?**  
*Why choose option 1?*
If you assume your system is able to infer, it might be covered by the AI Act (if the other conditions for having an AI system are also fulfilled), but if it is not prohibited or high-risk, the rules you need to follow are usually simple. Therefore, this approach makes sure you are on the safe side. 

AI systems must be able to figure out how to turn inputs into outputs. 'Inference' can be thought of as the AI systems ability to think. This inference capability can be present both when the AI system is being *developed* and when it is being *used*. 

During the **development phase**, an AI system can figure out algorithms or models from the data it is trained on. Think of it as the AI system getting a recipe of how the world works based on the training data it sees.  

During the **use phase**, an AI system must be able to figure out what to do with the inputs it receives. If you use an AI-system like ChatGPT, your prompt is your input, and ChatGPT must then figure out what to do with it. *That* is inference. 

Even if you are unsure whether your system can "infer," it is often wise to act as if it can. If a system is capable of inference, it may produce incorrect or biased results that could mislead users. By assuming your system can infer, you encourage users to remain cautious and critical when interpreting outputs, helping to prevent blind trust.

Beyond legal compliance, this approach also creates business value: it fosters a culture of responsible AI use, reduces the risk of reputational damage from incorrect outputs, and builds trust with customers and regulators, all of which contribute to long-term business credibility.

*Why choose option 2?*
You should choose option 2 only if your system does not infer (think). If your system is a simple, traditional software program (one that follows fixed rules (an "if/then" system)), you can  assume that it does not have inference capability.

Be aware, however, that many modern software systems that appear rule-based may still rely on AI components performing inference, for example by using pre-trained models to personalise outputs. This can be hard to spot, so if you are in doubt, you should ask your AI provider of the system's inference capability. 

If you wrongly assume that your system is not capable of inference, and it is subject to the AI Act, you risk  infringing on the AI Act, which could result in fines or other penalties. In addition, users may place unwarranted trust in outputs from a system that is actually performing inference without proper oversight. Taking the cautious approach helps protect both legal compliance and long-term trust in your products.

You should therefore choose option 2 only if you are absolutely certain that your system is unable to infer.


**When in doubt, assume your system is able to infer.** Better safe than sorry.
