If youâ€™re unsure whether your system can **adapt** or **change its behavior over time**, you have two choices:

1. **Assume it _can_ adapt after deployment.**

2. **Assume it _cannot_ adapt after deployment.**

We recommend you assume it _can_ (option 1).

**Why?**  
_Why choose option 1?_
Even though the AI Act does not require a system to adapt after deployment in order to be classified as an AI system, adaptiveness still matters greatly for risk management. If your system can change itself, the version you have today may behave very differently from the version you had last week.

Imagine deploying a low-risk marketing AI that initially seems harmless. Over time, the system updates itself and begins using aggressive or even prohibited tactics to boost results, potentially turning it into a prohibited AI system without your knowledge. Now you are exposed to serious legal and reputational risks.

Adaptive systems must therefore be monitored regularly. They evolve and if you are not watching, they may develop behaviours you did not approve of. Even if your system is ultimately not adaptive, building monitoring can create real business value. It can help you maintain consistent product quality and ensure that outputs remain aligned with customer expectations. This makes your system more attractive to enterprise buyers who expect robust governance.

To put this into perspective, consider ChatGPT. In 2022, we had ChatGPT 3, with around 175 billion parameters. In 2023, ChatGPT 4 was released with 1.8 trillion parameters (more than ten times more). The "same" system (ChatGPT) had become fundamentally different, without most people realising it. This illustrates how adaptiveness and system evolution can make a huge difference, which must be carefully managed.

Unless you are absolutely certain that your system does not change after deployment, it is safer to assume that it does.

_Why choose option 2?_
Whether your system can adapt or not does not affect whether it classifies as an AI system under the AI Act. If you are in doubt, you can therefore assume that your system does not exhibit adaptiveness without risking non-compliance with the AI Act.

However, if your system does adapt and you fail to monitor it appropriately, you may still face business risks, such as outputs that drift away from compliance, harm user trust, or lead to unintended behaviours. If in doubt, building in light monitoring is often a low-cost safeguard that can save your organisation from larger risks later.

**When in doubt, assume your system can adapt.** Better safe than sorry.
